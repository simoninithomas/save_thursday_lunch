# Introduction to Q-Learning [[introduction-q-learning]]

ADD THUMBNAIL

In theÂ first chapter of this class, we learned about Reinforcement Learning (RL), the RL process, and the different methods to solve an RL problem. We also **trained our first agents and uploaded them to the Hugging Face Hub.**

In this chapter, we're going toÂ **dive deeper into one of the Reinforcement Learning methods: value-based methods**Â and study our first RL algorithm:Â **Q-Learning.**

We'll alsoÂ **implement our first RL agent from scratch**: a Q-Learning agent and will train it in two environments:

1. Frozen-Lake-v1 (non-slippery version): where our agent will need toÂ **go from the starting state (S) to the goal state (G)**Â by walking only on frozen tiles (F) and avoiding holes (H).
2. An autonomous taxi will needÂ **to learn to navigate**Â a city toÂ **transport its passengers from point A to point B.**


<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif" alt="Environments"/>


We'll learn about the value-based methods and the difference between Monte Carlo and Temporal Difference Learning. And then, **we'll study and code our first RL algorithm**: Q-Learning, and implement our first RL Agent.

This unit is **fundamental if you want to be able to work on Deep Q-Learning**: the first Deep RL algorithm that was able to play Atari games and beat the human level on some of them (breakout, space invadersâ€¦).

So let's get started! ðŸš€
